{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747271fb-374f-4919-ac53-824666a70e10",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ab5cf5-c20c-497b-8dc1-3c95e9ba5158",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "It is a branch of AI that gives the machines the ability to process, understand and derive meaning from human languages. It combines the field of linguistics and computer science to decipher the language from **text** and **speech**.\n",
    "\n",
    "NLP techniques aim to build a bridge between humans and technology with the help of Language Modelling\n",
    "\n",
    "`Language modelling` is the core approach behind Natural Language Processing. This **statistical approach** analyzes the pattern in human language with the aim to predict the next word.\n",
    "\n",
    "A `language model` outputs a **probability distribution over words or word sequences**. A language model gives the probability that a certain word (or sequence of words) is \"valid\".\n",
    "\n",
    "In other words, and more future-oriented:\n",
    "\n",
    "NLP is a technique that enables ML algorithms to interpret and to understand the way humans communicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903203b3-b6ab-485a-93dc-ac892b940c94",
   "metadata": {},
   "source": [
    "## NLP tasks\n",
    "\n",
    "With the help of NLP techniques we can perform various tasks:\n",
    "\n",
    "- sentiment analysis\n",
    "- text classification & clustering (eg. spam detection, topic analysis)\n",
    "- chatbot and virtual assistants\n",
    "- translations\n",
    "- text summarisation\n",
    "- auto-correction of text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93be2a0-226c-4ee7-9e4c-43ff137741cc",
   "metadata": {},
   "source": [
    "## NLP libraries\n",
    "\n",
    "There are two most popular NLP libararies in python:\n",
    "- `nltk` - Natural Language Toolkit\n",
    "- `spaCy` - NLP library written in Python & Cython (C-extension for Python)\n",
    "\n",
    "Origins:\n",
    "\n",
    "- `nltk` was built by scholars and researchers whose aim was to create complex NLP funtions. It grounds a foundation for NLP algorithms.\n",
    "- `spaCy` is more often used in production, for business purposes. \n",
    "\n",
    "Pros/Cons:\n",
    "\n",
    "- `spaCy`- more modern, cleaner API, support for parallel processing, good documentation, faster, more efficient.\n",
    "\n",
    "- `nltk` - more tools, more packages - could be useful for a specific/niche task.\n",
    "\n",
    "Other interesting NLP libraries:\n",
    "\n",
    "- `TextBlob`- easier than `nltk`, good for sentiment analysis, comes with a native wrapper around the Google Translate API\n",
    "- Stanford `CoreNLP` - rich in features, very efficient and popular in production\n",
    "- `Gensim`- developed in Czech Republic, powerful and scalable library\n",
    "- `vaderSentiment`- lexicon and rule-based sentiment analysis tool\n",
    "- `flair` - simple framework for NLP projects developed by Humboldt Uni in Berlin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0654f75-8994-422f-a8e8-afbc863644d6",
   "metadata": {},
   "source": [
    "### Fundamental processing steps/skills in NLP\n",
    "\n",
    "In order to derive meaning from text and get a disired response, we need to follow specific steps. These are core skills & steps involved in Natural Language Processing:\n",
    "\n",
    "**1. Pre-processing steps** - almost always the text that we get to work with is not 'clean' and ready to be processed. Typical pre-processing steps in NLP are:\n",
    "    \n",
    "- Cleaning data from irrelevant information\n",
    "- Removing stop words\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "Notice, that depending on your NLP approach it might not be neccessary to perform above steps because:\n",
    "- they might not fit your modelling objective (eg. stopwords might be important for your analysis)\n",
    "- or because the numerical word represention approach (see step No. 3) does it automatically for you.\n",
    "\n",
    "**2. Tokenizing** - breaking down the text corpus into single words. Singular Words and their relationships are at the core of NLP\n",
    "\n",
    "**3. Assigning numerical representation to words (tokens)**\n",
    "\n",
    "- Bag-of-words approach\n",
    "- Word Embeddings approach\n",
    "\n",
    "**4. Choosing & applying appropriate ML algorithm for a given NLP task**\n",
    "\n",
    "**Other useful skills and techniques:**\n",
    "\n",
    "- Part of Speech Tagging (POS Tagging)\n",
    "- Named Entity Tagging\n",
    "- REGEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1320a47f-6207-471e-81c8-185256cd86e2",
   "metadata": {},
   "source": [
    "Performing NLP tasks is virtually impossible without knowledge of `regex` -  sequence of characters that specifies a search pattern in text. Let's dive into REGEX as a warm up before doing NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea0ea7e-1e61-4d48-8da2-22a825e1fb02",
   "metadata": {},
   "source": [
    "# NLP skills & techniques\n",
    "## Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc0e6a-21fd-4874-9b9d-eabee26655c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Knowledge of Regex is a prerequisite for working with textual data.\n",
    "\n",
    "Regular expressions in Python is essentialy a small programming language embedded inside Python.\n",
    "\n",
    "REGEX answers the question:\n",
    "- \"Does this piece of text match the pattern\"?\n",
    "- \"Is there a match for a pattern anywhere in a string\"?\n",
    "\n",
    "#### Simplest Regex case - match characters\n",
    "\n",
    "re is an in-built Python module. It has four key methods used to look for a pattern in a string:\n",
    "- `.finditer()`\n",
    "- `.match()`\n",
    "- `.findall()`\n",
    "- `.search()`\n",
    "\n",
    "and two methods to modify the string\n",
    "\n",
    "- `.split()`\n",
    "- `.sub()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "701a20f2-21e1-4f6b-a332-c3fec37a6c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(3, 6), match='abc'>\n",
      "<re.Match object; span=(12, 15), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "test = \"123abc456789abc123ABC\"\n",
    "\n",
    "#Let's look for 'abc' or in the above text\n",
    "\n",
    "pattern = re.compile(r\"abc\") \n",
    "matches = pattern.finditer(test)\n",
    "\n",
    "#matches = re.finditer(r\"abc\", test)\n",
    "\n",
    "for match in matches:\n",
    "    print(match)\n",
    "    \n",
    "#notice that regular expression is case-sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873563c5-0490-42d3-a699-3a9e2449dc44",
   "metadata": {},
   "source": [
    "> ### TASK\n",
    "\n",
    "Apply .match() and .search() method on pattern. What is the output and what does it mean? *You do not need for loop for those two functions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd9e8dc-0fa7-4a3f-a5d9-f1a4e2fcbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.match() method determines whether the pattern matches at the beginning of the string\n",
    "#.search() method looks for the first occurence of the pattern in a string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc16ba5-5654-478c-9755-f6c9020115b8",
   "metadata": {},
   "source": [
    "#### `raw` vs. regular `string`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e80ac-5770-4103-bb8a-94dcd68ae596",
   "metadata": {},
   "source": [
    "The above string is a raw string, it has no escape characters. `raw` string treat all characters as literal characters.\n",
    "\n",
    "Notice the difference between `raw` and just `string`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd6eba-9abf-4966-82af-479680ed9dec",
   "metadata": {},
   "source": [
    "**regular `string`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e136b7c-f2de-4304-9921-2f61cb5c9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "a ='Hello\\nMr\\nProfessor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d577a40-e31b-4b62-ae18-2a46fc1cc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Mr\n",
      "Professor\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694161e7-8d7f-49db-92f3-89ee359152b6",
   "metadata": {},
   "source": [
    "**`raw` string**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4211b819-e28b-4cef-bd22-a78393201cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\\nMr\\nProfessor\n"
     ]
    }
   ],
   "source": [
    "b = r\"Hello\\nMr\\nProfessor\"\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b725561b-2e16-4e69-ad09-0eb5fb1b7156",
   "metadata": {},
   "source": [
    "`r` prefix changes how the string literal is interpreted. Without the `r`, backslashes are treated as **escape characters**. With `r`, backslashes are treated as literal. The type stays the same - `str` or `object`\n",
    "\n",
    "You might want to always make sure to use raw version of string in order to catch all characters literally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed5cad-aae7-496f-9bd0-a4992589ac81",
   "metadata": {},
   "source": [
    "#### Escape characters\n",
    "#### What is the point of escape characters?\n",
    "\n",
    "Escape characters - backlash (\\) followed by the desired character is a way to allow illegal characters in a string or indicate special characters like new line or tab. For example `\"` is an illegal character in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02d98f8a-926a-4d4c-8a77-bd3418a8be01",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1686009549.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [18]\u001b[0;36m\u001b[0m\n\u001b[0;31m    txt = \"My friends used to call her \"Lola\" when she was young\"\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "txt = \"My friends used to call her \"Lola\" when she was young\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f11e3-34b9-4f45-8c5d-793be6432d8b",
   "metadata": {},
   "source": [
    "In order to use \"\" in a string you need to escape it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "095123ad-849d-45ea-866e-27c1a1c9d3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friends used to call her \"Lola\" when she was young\n"
     ]
    }
   ],
   "source": [
    "txt = \"My friends used to call her \\\"Lola\\\" when she was young\"\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a82569e-2859-48d8-af7e-bde665dcf4e9",
   "metadata": {},
   "source": [
    "Popular escaped characters:\n",
    "\n",
    "- `\\n` - new line\n",
    "- `\\t` - tabulator\n",
    "- `\\\\` - single backlash\n",
    "- `\\\"` - quotation mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe7d31d-754a-42c4-a21f-5af2d4b68ff0",
   "metadata": {},
   "source": [
    "> ### TASK\n",
    "\n",
    "1. assign a string *\"My friends used to call her \"Lola\" when she was young\"* to a new variable called \"new_string\"\n",
    "2. search for \"Lola\" pattern in this string - quotation marks `\"` need to be included in your output. \n",
    "Experiment with finditer(), findall() and search() method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e831607-8496-42ba-bb26-5334820721be",
   "metadata": {},
   "source": [
    "#### Meta characters in regular expressions\n",
    "\n",
    "Metacharacters are the building blocks of RegEx. Characters in RegEx are understood to be either a metacharacter that has a special meaning or just a regular character with a literal meaning.\n",
    "\n",
    "In other words, Meta characters in regex have **special meaning**\n",
    "\n",
    "- `\\` Marks the next character as either a special character or a literal. For example, n matches the character n, whereas \\n matches a newline character. The sequence \\\\\\ matches \\ and \\\\( matches (.\n",
    "\n",
    "- `^` Matches the beginning of input\n",
    "- `$` Matches the end of input\n",
    "- `*` Matches the preceding character zero or more times. For example, zo* matches either z, zo or zoo.\n",
    "- `+` Matches the preceding character one or more times. For example, zo+ matches zo, zoo but not z.\n",
    "- `?` Matches the preceding character zero or one time. For example, a?ve? matches the ve in never.\n",
    "- `.` Any character (except a newline character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f02a4202-12e8-4ddb-9eb8-b470003943ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ',',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =\"hello world, hello world\"\n",
    "re.findall(\".\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7353df1e-b289-4473-ada5-03dda04dbb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'hello']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =\"hello world, hello world\"\n",
    "re.findall(\"hello\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b5f38c6-53e4-4948-8e1f-418458ec56b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a =\"hello world, hello world\"\n",
    "re.findall(\"^hello\", a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd403b1-c9b3-46ef-b20f-a5f26f7d4f5f",
   "metadata": {},
   "source": [
    "#### What do we need `regex`for in data analysis?\n",
    "\n",
    "Three key useful methods that leverage regex in data analysis and data cleaning:\n",
    "- `str.contains()`\n",
    "- `str.extract()`\n",
    "- `str.replace()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22711a9f-bd8a-4569-9a35-7103de4d734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39066756-289e-4945-985b-eecbaf8c1e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alicia Johnson</td>\n",
       "      <td>28 May 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Patt\\Robinson</td>\n",
       "      <td>13 June 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sean Pean</td>\n",
       "      <td>09 March 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Patt</td>\n",
       "      <td>14 May 1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name       birthday\n",
       "0        Alicia Johnson    28 May 2000\n",
       "1  Robert Patt\\Robinson   13 June 1967\n",
       "2             Sean Pean  09 March 2005\n",
       "3             Alex Patt    14 May 1999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'name' : [\"Alicia Johnson\", \"Robert Patt\\\\Robinson\", \"Sean Pean\", \"Alex Patt\"], \n",
    "                   'birthday' : [\"28 May 2000\", \"13 June 1967\", \"09 March 2005\", \"14 May 1999\"]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3397d-9745-4ad2-82c5-662427910a9a",
   "metadata": {},
   "source": [
    "**`str.contains()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a888b-db20-4534-86fd-22fc9d83d68f",
   "metadata": {},
   "source": [
    "Return persons with a lastname \"Patt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05e636f8-36c7-4989-b785-08ad1a8b830f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3     True\n",
       "Name: name, dtype: bool"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'].str.contains('Patt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e9062d0-06e9-41a3-bb49-d7c909ee62c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birthday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Patt\\Robinson</td>\n",
       "      <td>13 June 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Patt</td>\n",
       "      <td>14 May 1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name      birthday\n",
       "1  Robert Patt\\Robinson  13 June 1967\n",
       "3             Alex Patt   14 May 1999"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['name'].str.contains('Patt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae7d56-d0fa-48d4-9a44-7435a2b4f29e",
   "metadata": {},
   "source": [
    "**`str.replace()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "673c28ff-278e-47d2-a15d-5eee02d59ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Robert Patt\\\\Robinson'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3b0fa04-b317-4411-ac2b-c413827e8f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birthday</th>\n",
       "      <th>new_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alicia Johnson</td>\n",
       "      <td>28 May 2000</td>\n",
       "      <td>Alicia Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Patt\\Robinson</td>\n",
       "      <td>13 June 1967</td>\n",
       "      <td>Robert Patt Robinson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sean Pean</td>\n",
       "      <td>09 March 2005</td>\n",
       "      <td>Sean Pean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Patt</td>\n",
       "      <td>14 May 1999</td>\n",
       "      <td>Alex Patt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name       birthday              new_name\n",
       "0        Alicia Johnson    28 May 2000        Alicia Johnson\n",
       "1  Robert Patt\\Robinson   13 June 1967  Robert Patt Robinson\n",
       "2             Sean Pean  09 March 2005             Sean Pean\n",
       "3             Alex Patt    14 May 1999             Alex Patt"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new_name'] = df['name'].str.replace(r\"\\\\\", \" \", regex = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de95a02-b8bc-40a1-881f-7f9845a9cdbc",
   "metadata": {},
   "source": [
    "**`str.extract()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8e25e-752c-4b8a-b632-55d0c9cf4470",
   "metadata": {},
   "source": [
    "Create a new column with only year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9cfc075-1337-42a5-a377-79c4355c6365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birthday</th>\n",
       "      <th>new_name</th>\n",
       "      <th>birth_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alicia Johnson</td>\n",
       "      <td>28 May 2000</td>\n",
       "      <td>Alicia Johnson</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Robert Patt\\Robinson</td>\n",
       "      <td>13 June 1967</td>\n",
       "      <td>Robert Patt Robinson</td>\n",
       "      <td>1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sean Pean</td>\n",
       "      <td>09 March 2005</td>\n",
       "      <td>Sean Pean</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alex Patt</td>\n",
       "      <td>14 May 1999</td>\n",
       "      <td>Alex Patt</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name       birthday              new_name birth_year\n",
       "0        Alicia Johnson    28 May 2000        Alicia Johnson       2000\n",
       "1  Robert Patt\\Robinson   13 June 1967  Robert Patt Robinson       1967\n",
       "2             Sean Pean  09 March 2005             Sean Pean       2005\n",
       "3             Alex Patt    14 May 1999             Alex Patt       1999"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['birth_year'] = df['birthday'].str.extract(r'(.{4}$)', expand=False).str.strip()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0863f-ff08-4446-9172-eba9d0611651",
   "metadata": {},
   "source": [
    "> ### TASK\n",
    "\n",
    "Create a new column - \"day\" and extract only the day from the \"birthday\" column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00fdcb8-9b70-4b1e-b7cf-a92b5c44680d",
   "metadata": {},
   "source": [
    "This is just a basic regex tutorial, check regex cheat sheets and experiment with regex using this tool: https://regex101.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f46385-63b5-47d6-b15e-d034c5cf7144",
   "metadata": {},
   "source": [
    "# Text classification task\n",
    "\n",
    "Let's get back to the core and get to know standard NLP techniques.\n",
    "\n",
    "We will first explore a simple text classification task and get familiar with the overall handling of text in NLP and suitable algorithm.\n",
    "\n",
    "The core in NLP is a \"word\" - that is why the first step in NLP is to `tokenize`our text - extract singular words. After tokenizing, we can choose the approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cfbd1-dcd0-483a-a854-04810a349116",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bag of words approach\n",
    "\n",
    "Why do we need this approach? When performing NLP tasks, we deal with numerical vectors. Since text is not numerical, we need to find a way to convert it and bag-of-words approach comes in handy. Bag of words helps us convert textual data into numerical representation.\n",
    "\n",
    "Imagine we would like to create an algorithm that classify user reviews into two categories:\n",
    "\n",
    "- reviews about pizza\n",
    "- reviews about rice\n",
    "\n",
    "1. \"I love this pizza\"\n",
    "2. \"The dough is delicious\"\n",
    "3. \"I love this sticky rice\"\n",
    "4. \"The rice is delicios\"\n",
    "\n",
    "Bag-of-words approach will extract each unique word from all reviews in the following way:\n",
    "\n",
    "\"I love this pizza The dough is delicious sticky rice.\"\n",
    "\n",
    "Let's implement bag-of-words with scikit-learn\n",
    "\n",
    "scikit-learn provides utilities for the most common ways to extract numerical features from text content: https://scikit-learn.org/stable/modules/feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5703c16-7166-49dd-a7fe-22150e7826ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train =  [\"I love this pizza\", \"The pizza is not good\", \"I love this delicious sticky rice\", \"The rice is very very good\"] #in NLP terminology you could call this 'corpus'\n",
    "\n",
    "#corpus =  [\"I love this pizza\", \"The dough is delicious\", \"I love this sticky rice\", \"The rice is delicious\"]\n",
    "\n",
    "#Let's transform the list into a vector representation\n",
    "\n",
    "vectorizer = CountVectorizer() #tokenizing happens in the background\n",
    "\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c28f46d-1cea-46b4-9334-65255eea5246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "019aeb19-7b53-4934-bfec-0b148e99ccfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delicious' 'good' 'is' 'love' 'not' 'pizza' 'rice' 'sticky' 'the' 'this'\n",
      " 'very']\n",
      "[[0 0 0 1 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 1 1 0 0 0 1 0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_train_vectors.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86acffff-9074-4a21-b041-1c1f27f9aa5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>delicious</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>not</th>\n",
       "      <th>pizza</th>\n",
       "      <th>rice</th>\n",
       "      <th>sticky</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this pizza</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The pizza is not good</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love this delicious sticky rice</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The rice is very very good</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         review_text  delicious  good  is  love  not  pizza  \\\n",
       "0                  I love this pizza          0     0   0     1    0      1   \n",
       "1              The pizza is not good          0     1   1     0    1      1   \n",
       "2  I love this delicious sticky rice          1     0   0     1    0      0   \n",
       "3         The rice is very very good          0     1   1     0    0      0   \n",
       "\n",
       "   rice  sticky  the  this  very  \n",
       "0     0       0    0     1     0  \n",
       "1     0       0    1     0     0  \n",
       "2     1       1    0     1     0  \n",
       "3     1       0    1     0     2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's make a dataframe - Just for visual representation\n",
    "\n",
    "bow_df = pd.DataFrame(X_train_vectors.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "bow_df.insert(0, 'review_text', X_train)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720cf06f-340b-4249-9105-a0726002219a",
   "metadata": {},
   "source": [
    "By default, **CountVectorizer** is non binary - notice 'very' is assigned value 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3714b-1f5b-437e-894c-bea4c238cc9a",
   "metadata": {},
   "source": [
    "**Vectorization** is the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (`tokenization`, `counting` and `normalization`) is called the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences while completely **ignoring the relative position of the words in the document**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c45f23-9c64-40aa-9f92-902de86ff72d",
   "metadata": {},
   "source": [
    "### Train your first NLP model\n",
    "\n",
    "NLP is a technique that enables ML algorithms to interpret and to understand the way humans communicate.\n",
    "\n",
    "Remember that in order to perform a given ML task (eg. sentiment classification or text classification) `ML algorithms` process `data`, then create a `ML model`, apply it on `unseen dataset` and generate `result`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421afa64-7c96-4040-89d4-621a12a044ef",
   "metadata": {},
   "source": [
    "![algorithm-Copy1](algorithm-Copy1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c19d99-7c13-470c-b834-91e97556d7ea",
   "metadata": {},
   "source": [
    "Assign labels to your sentences in order to train the model. Let's first visualise it in a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e6dac56-8cfe-47be-ab1e-f217be0d2771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>delicious</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>not</th>\n",
       "      <th>pizza</th>\n",
       "      <th>rice</th>\n",
       "      <th>sticky</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this pizza</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The pizza is not good</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love this delicious sticky rice</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The rice is very very good</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         review_text  delicious  good  is  love  not  pizza  \\\n",
       "0                  I love this pizza          0     0   0     1    0      1   \n",
       "1              The pizza is not good          0     1   1     0    1      1   \n",
       "2  I love this delicious sticky rice          1     0   0     1    0      0   \n",
       "3         The rice is very very good          0     1   1     0    0      0   \n",
       "\n",
       "   rice  sticky  the  this  very  label  \n",
       "0     0       0    0     1     0  pizza  \n",
       "1     0       0    1     0     0  pizza  \n",
       "2     1       1    0     1     0   rice  \n",
       "3     1       0    1     0     2   rice  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = [\"pizza\",\"pizza\",\"rice\",\"rice\"]\n",
    "bow_df[\"label\"] = y_train\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75377bd3-c86b-4952-8abc-45654f8dd78c",
   "metadata": {},
   "source": [
    "We are about to train our model. Which of the columns are features and which are labels?\n",
    "\n",
    "Let's build a simple classifier, to classify which review belongs to which category - pizza or rice? We will use Support Vector Machines - namely Support Vector Classification:\n",
    "\n",
    "### Support Vector Classification\n",
    "\n",
    "How does this ML algorithm work? \n",
    "\n",
    "SVMs finds a separating line(or hyperplane) between data points of two classes. `SVM algorithm` takes the data as an input and finds a line that separates the two categories.\n",
    "\n",
    "Let's start with a two-dimensional problem. Suppose you have two categories and you want to correctly classify a new observation that will appear on the chart. With SVM you are finding a line - Optimal Hyperplane that will have the largest Maximum Margin between the two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b43696-711e-443f-9d55-12a6a6fb2e95",
   "metadata": {},
   "source": [
    "![svm](svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599f64f1-a51e-4128-85ca-d8e596f035b6",
   "metadata": {},
   "source": [
    "If your data doesn't allow to draw a clear line because your data points overlap, you can still use a Kernel Trick. Details: https://towardsdatascience.com/svm-feature-selection-and-kernels-840781cc1a6c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e89ed25a-9caa-491f-8a25-4fd760d9aaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.SVC(kernel = 'linear', probability = True)\n",
    "clf_svm.fit(X_train_vectors, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea10f5a-39a2-4d38-958d-476817a6bac3",
   "metadata": {},
   "source": [
    "Let's predict a label for a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffb26903-470d-4fd9-86b7-d53f1194b9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pizza'], dtype='<U5')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform([\"This pizza is great\"]) # Try with \"this pizza is delicious\"\n",
    "clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f9627ae-c332-430d-89d8-0746dbc734f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55565672, 0.44434328])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.predict_proba(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12ed43-a974-47ff-b174-2483b8446b71",
   "metadata": {},
   "source": [
    "#### Bag of n-grams approach\n",
    "\n",
    "In some cases you might want to capture and count more than just one token. We can use n-grams, meaning expressions that consist of n number of words. Let's do bag of words again, but this time with bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8979b30d-a3f1-4220-ae81-6b47fbb9f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delicious' 'good' 'is' 'love' 'not' 'pizza' 'rice' 'sticky' 'the' 'this'\n",
      " 'very']\n",
      "[[0 0 0 1 0 1 0 0 0 1 0]\n",
      " [0 1 1 0 1 1 0 0 1 0 0]\n",
      " [1 0 0 1 0 0 1 1 0 1 0]\n",
      " [0 1 1 0 0 0 1 0 1 0 2]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>delicious</th>\n",
       "      <th>delicious sticky</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>is not</th>\n",
       "      <th>is very</th>\n",
       "      <th>love</th>\n",
       "      <th>love this</th>\n",
       "      <th>not</th>\n",
       "      <th>...</th>\n",
       "      <th>sticky rice</th>\n",
       "      <th>the</th>\n",
       "      <th>the pizza</th>\n",
       "      <th>the rice</th>\n",
       "      <th>this</th>\n",
       "      <th>this delicious</th>\n",
       "      <th>this pizza</th>\n",
       "      <th>very</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this pizza</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The pizza is not good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love this delicious sticky rice</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The rice is very very good</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         review_text  delicious  delicious sticky  good  is  \\\n",
       "0                  I love this pizza          0                 0     0   0   \n",
       "1              The pizza is not good          0                 0     1   1   \n",
       "2  I love this delicious sticky rice          1                 1     0   0   \n",
       "3         The rice is very very good          0                 0     1   1   \n",
       "\n",
       "   is not  is very  love  love this  not  ...  sticky rice  the  the pizza  \\\n",
       "0       0        0     1          1    0  ...            0    0          0   \n",
       "1       1        0     0          0    1  ...            0    1          1   \n",
       "2       0        0     1          1    0  ...            1    0          0   \n",
       "3       0        1     0          0    0  ...            0    1          0   \n",
       "\n",
       "   the rice  this  this delicious  this pizza  very  very good  very very  \n",
       "0         0     1               0           1     0          0          0  \n",
       "1         0     0               0           0     0          0          0  \n",
       "2         0     1               1           0     0          0          0  \n",
       "3         1     0               0           0     2          1          1  \n",
       "\n",
       "[4 rows x 26 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_ngram = CountVectorizer(ngram_range = (1,2))\n",
    "\n",
    "X_train_vectors_ngram = vectorizer_ngram.fit_transform(X_train)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X_train_vectors.toarray())\n",
    "\n",
    "bow_df = pd.DataFrame(X_train_vectors_ngram.toarray(), columns = vectorizer_ngram.get_feature_names_out())\n",
    "bow_df.insert(0, 'review_text', X_train)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f4a0a2-e846-4e3d-a536-c570a518e34d",
   "metadata": {},
   "source": [
    "#### Limitation of bag of words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68574acd-a209-444d-b69a-d7a858259e8d",
   "metadata": {},
   "source": [
    "If a model haven't seen a word before, it won't be able to link it to correct category. The only information we provide are the words and occurences in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34bac3ff-2e94-4c15-9aba-3355b908c3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice'], dtype='<U5')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = vectorizer.transform([\"Those pizzas' dough is very good\"]) # \n",
    "clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86f38997-3ab6-4afc-af3d-aa032183551d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4791528, 0.5208472])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.predict_proba(X_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059a33a-4310-4da1-8739-0ea612419d4e",
   "metadata": {},
   "source": [
    "In order to make Bag of Words approach more accurate you can apply a TFIDF transformation. Additional materials:\n",
    "\n",
    "- SVM explained: https://www.youtube.com/watch?v=efR1C6CvhmE&ab_channel=StatQuestwithJoshStarmer\n",
    "- TFIDF in sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "- TFIDF explained: https://monkeylearn.com/blog/what-is-tf-idf/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25f8c2d-d606-4c95-9f0a-f30f66e08c25",
   "metadata": {},
   "source": [
    "### Problem of Sparsity\n",
    "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them).\n",
    "\n",
    "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in #total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the scipy.sparse package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbeada-59e3-4c11-9290-58a967b87715",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29be0c7-24a4-4aaf-9347-269658836b0c",
   "metadata": {},
   "source": [
    "Word Embeddings approach converts text into numerical representation and aims to capture **semantic meaning of a word**.\n",
    "\n",
    "Word embeddings are **word vector representations** in which words with similar semantic meaning have similar representation. Word vectors are one of the most efficient ways to represent words.\n",
    "\n",
    "In general, word vectors are much better in representing words than one hot encoded vectors. In bag of words approach, the vectors for “pizza” and “dough” are as close together as \"pizza\" and \"forest\" - the words in Bag of Words are completely isolated entities.\n",
    "\n",
    "Check the following examples:\n",
    "\n",
    "- \" The **pizza** is made to perfection\"\n",
    "- \" **Pizza**'s **dough** is very delicious\"\n",
    "- \"The **dough** should have stayed longer in the **oven**\"\n",
    "\n",
    "In Word embeddings if pizza and dough are related, dough and oven are related, then pizza and oven will also be related (in great simplification this is what happens in word embeddings approach)\n",
    "\n",
    "Let's use spaCy library to generate word vectors: https://spacy.io/usage/linguistic-features#vectors-similarity\n",
    "\n",
    "We will not train Word Vector from scratch, we can take advantage of existing Word Embeddings.\n",
    "\n",
    "Two main methods in word embeddings:\n",
    "\n",
    "- continuos bag of words\n",
    "- skip gram\n",
    "\n",
    "If you are interested how the Word Embeddings are computed, check this article out: https://medium.com/analytics-vidhya/word-embeddings-in-nlp-word2vec-glove-fasttext-24d4d4286a73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0b2e47d-ba46-41a3-a0b8-deacfda33f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea59a0fb-aae2-4195-8ae4-7e5ca4e11e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4c49111c-fd1a-45d0-a557-839954443ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\") #Save Language Class in nlp variable (nlp is a convention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a97538db-175f-4aa9-a1f9-4bed8721d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This pizza is delicious\") #nlp class processes the text and saves it to doc object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9925986-0f9b-4410-a7be-4fee1ecb1030",
   "metadata": {},
   "source": [
    "Now we can access various elements of the doc object - The doc object owns the sequence of tokens and all their annotations. Let's check the vector for pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "989e0234-8c19-4e7e-863f-97859a0bbc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.7467e-01, -1.8932e-01,  5.8039e-01, -2.2462e-01,  9.0182e-02,\n",
       "        1.0529e+00, -6.3681e-01,  1.6173e-01,  9.5415e-01,  7.0889e-01,\n",
       "       -5.7771e-01,  6.4541e-01, -5.5105e-01, -4.7180e-01,  2.5780e-01,\n",
       "       -5.1330e-01,  1.1842e-01,  1.3074e+00,  5.7581e-02, -4.5192e-02,\n",
       "        4.2439e-01, -3.8203e-01,  9.4305e-02, -6.0597e-02, -3.8719e-01,\n",
       "       -6.3133e-01, -1.8201e-01,  2.3236e-01,  4.9453e-01, -1.1618e+00,\n",
       "        2.7373e-01,  5.3102e-01,  5.3476e-01, -7.9732e-01,  4.2004e-02,\n",
       "        2.2655e-01,  1.6060e-02,  2.8268e-02,  5.4367e-01,  8.4875e-01,\n",
       "        1.2247e-01, -1.2477e-01,  3.2317e-02, -9.0382e-02,  1.6604e-01,\n",
       "        7.4026e-01,  5.3353e-02,  5.4560e-01,  3.2068e-01,  1.8807e-01,\n",
       "        1.8679e-01, -3.1118e-01,  1.4894e-01,  4.0011e-01,  2.3684e-01,\n",
       "       -7.6084e-01,  3.9626e-01, -9.1346e-03,  1.3314e-01,  4.6786e-01,\n",
       "        1.6860e-01, -3.7035e-01, -2.0765e-01,  1.5869e-01, -1.5605e-01,\n",
       "       -4.1760e-01,  4.1834e-01,  2.8982e-01, -3.6611e-01,  5.8943e-01,\n",
       "       -6.9879e-02,  7.7475e-01, -3.2586e-01,  1.3027e-01, -9.7918e-01,\n",
       "       -1.4996e-01,  4.9167e-01, -1.1487e-01, -2.6248e-01, -4.7739e-01,\n",
       "       -3.7325e-01, -4.6275e-01, -3.7441e-02, -2.7094e-01, -1.0892e-01,\n",
       "       -4.2739e-01,  1.4097e+00,  6.3615e-01, -2.7136e-01,  3.2677e-01,\n",
       "       -5.4801e-01, -6.2330e-01,  3.4840e-01,  7.0775e-02,  1.4289e-01,\n",
       "        6.4907e-01, -3.9130e-01,  1.8515e-01,  1.5752e-01,  3.4807e-01,\n",
       "        1.7310e-01, -1.9448e-01,  3.5372e-01,  4.0014e-01,  1.1068e-01,\n",
       "       -5.8670e-01,  3.7281e-01, -3.6753e-01,  7.7636e-01, -2.5102e-01,\n",
       "       -4.5033e-01, -8.7571e-01, -6.3858e-01, -8.8223e-02,  4.0961e-01,\n",
       "        1.7726e-03,  4.0168e-01, -2.1038e-01,  8.1913e-02,  2.0615e-01,\n",
       "       -4.4484e-02,  1.9630e-01, -5.5967e-01, -2.0796e-01,  3.4159e-01,\n",
       "       -1.1311e-01,  2.8216e-01, -4.8118e-01,  2.4309e-01,  2.8144e-01,\n",
       "       -5.4665e-01, -3.7599e-01,  2.1855e-01,  2.6972e-01, -5.3495e-01,\n",
       "       -4.8415e-01, -4.4377e-01, -5.4330e-01, -1.7207e-01, -5.9925e-02,\n",
       "       -2.5616e+00,  2.2555e-01,  6.4467e-01,  8.4926e-01,  2.2532e-01,\n",
       "       -6.5575e-01, -4.9619e-01,  8.0017e-01,  6.0575e-01, -1.8785e-01,\n",
       "        5.0817e-01, -1.6219e-01,  6.3902e-01, -7.6987e-02, -4.7412e-01,\n",
       "       -8.6880e-01, -3.3538e-01, -2.4878e-01,  2.3402e-01,  2.1491e-01,\n",
       "       -3.8927e-01,  9.5099e-01, -1.2064e-01,  3.0635e-01, -2.0925e-01,\n",
       "       -6.7306e-01,  1.5134e-01, -1.9411e-01,  2.1034e-01,  4.0907e-02,\n",
       "        6.9762e-01,  1.7232e-01,  2.1728e-01, -6.8332e-01, -9.7306e-01,\n",
       "       -4.7116e-01,  5.3044e-01, -1.7278e-01,  9.8562e-02, -4.1927e-01,\n",
       "       -4.6342e-01, -2.3264e-01,  9.1713e-02, -2.2143e-01, -3.7354e-01,\n",
       "       -2.3373e-01, -2.1422e-01, -7.3157e-01, -4.9446e-01, -4.8978e-01,\n",
       "       -5.1552e-01,  3.8562e-01,  1.7507e-01,  1.7041e-01, -1.7717e-02,\n",
       "       -3.5705e-01,  7.1529e-01,  4.0581e-01, -1.2380e-01,  9.8469e-02,\n",
       "       -1.3574e-01,  9.3042e-03, -3.0743e-01,  2.3112e-01,  3.9722e-01,\n",
       "       -9.5411e-02, -1.0505e-01,  1.1369e-01,  8.4162e-01,  8.4086e-01,\n",
       "       -2.8105e-01, -4.0722e-02,  2.6649e-01, -1.5161e-01, -1.1414e-01,\n",
       "        1.7592e-01,  1.6247e-02, -2.8725e-01, -3.7412e-01,  4.9189e-01,\n",
       "       -8.3890e-01,  4.8307e-01, -2.1180e-01,  2.5899e-01,  5.0550e-01,\n",
       "       -7.0935e-01, -4.5606e-01, -8.1236e-02, -4.8558e-01,  2.8385e-01,\n",
       "       -5.7435e-01, -6.8628e-02, -3.0792e-01, -3.7178e-01, -1.9695e-01,\n",
       "       -2.1019e-01, -2.4515e-01,  1.0728e-02,  3.1657e-01,  1.5115e-01,\n",
       "       -3.6303e-02,  1.8256e-02, -4.1968e-01, -8.2135e-01,  3.8109e-01,\n",
       "       -1.4129e-01, -4.7227e-01, -8.7706e-01, -4.9720e-01, -7.4849e-01,\n",
       "       -1.6131e-02,  1.4916e-01, -7.6933e-01,  4.1800e-01, -2.4896e-01,\n",
       "        6.5297e-01, -6.1281e-02, -9.5284e-02, -2.8729e-01,  1.8957e-01,\n",
       "        1.2246e-01, -5.1908e-01, -2.3579e-01, -1.3992e-01,  2.4308e-01,\n",
       "        5.7350e-01, -1.2777e-01, -4.6454e-01, -9.0755e-01, -1.0540e-01,\n",
       "        7.0861e-01,  2.1350e-01, -2.3852e-01,  2.2534e-02, -5.1926e-01,\n",
       "        1.0419e-01,  1.0880e-01, -3.5305e-01, -2.0128e-01,  7.7258e-01,\n",
       "        7.5041e-01,  1.3464e-01,  2.1490e-01, -6.9045e-01,  1.7362e-02,\n",
       "        5.4047e-01,  7.5555e-01,  7.4757e-01, -3.7845e-02, -1.4689e-01,\n",
       "        7.1482e-01,  3.1973e-02, -7.3137e-01, -2.2230e-01, -3.1676e-01,\n",
       "       -3.8215e-01, -2.1487e-01, -1.0230e+00,  3.3344e-01,  1.6836e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7509f4-40e5-483a-83a0-a893b59dabf0",
   "metadata": {},
   "source": [
    "Now, lets compute Word Embeddings for our small X_train dataset. We get averaged values for each sentence from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f969c45d-a5a6-4ec9-acdc-124233e46461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this pizza', 'The pizza is not good', 'I love this delicious sticky rice', 'The rice is very very good']\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08af49d7-3611-44d4-a1f2-749f1a13a351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I love this pizza"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [ nlp(text) for text in X_train]\n",
    "\n",
    "#print(docs[0].vector)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "71fc97a4-d81a-4c38-a07d-9c71847af1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_embeddings = [ doc.vector for doc in docs]\n",
    "\n",
    "clf_svm_embeddings = svm.SVC(kernel = 'linear', probability = True)\n",
    "clf_svm_embeddings.fit(X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "327dfd92-322e-44f5-8679-10d946bc914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pizza'], dtype='<U5')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = [\"Those pizzas dough is very good\"]\n",
    "test_docs = [nlp(text) for text in X_test]\n",
    "test_embeddings = [doc.vector for doc in test_docs]\n",
    "\n",
    "clf_svm_embeddings.predict(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab9eaf3a-403d-4f9a-aa9a-4f928c23b27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51250099, 0.48749901])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm_embeddings.predict_proba(test_embeddings)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e72e4-e11f-45e4-b3ae-df91de01d972",
   "metadata": {},
   "source": [
    "Word embeddings drawbacks:\n",
    "\n",
    "- words with multiple meanings are conflated into a single representation (bank - institution or place to sit, light - in terms of colour or in terms of lightweight, easy)\n",
    "\n",
    "- with averaging random outliers, extreme values cause problems. Word Vectors might not be the best choice after all for classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dc49a3-e1ff-472b-9afb-cefe6d3b7e60",
   "metadata": {},
   "source": [
    "## Let's explore NLP pre-processing techniques\n",
    "\n",
    "- Removing stop words\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- POS tagging\n",
    "\n",
    "Remember that oftentimes you do not need to perform those steps yourself because it's not needed, or the process is happening under the hood of the algorithm.\n",
    "\n",
    "### Define stop words\n",
    "\n",
    "In some cases, we might not be interested in including some words in our analysis. We define a list of unwanted words and call the list stopwords. Example stopwords are: its, an, the, for, and that, you.\n",
    "\n",
    "> Before defining & removing stopwords, you need to tokenize your sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72966613-ff1a-49a4-8fb9-42d0b505aba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tokenize import word_tokenize\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "text = \"Let's check which stopwords will be removed from this sentence\"\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stripped_text = []\n",
    "for word in words:\n",
    "  if word not in stop_words:\n",
    "    stripped_text.append(word)\n",
    "\n",
    "\" \".join(stripped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba707b-4d4c-4c73-970b-57418c3e77dd",
   "metadata": {},
   "source": [
    "Load a pre-existing stopwords  from stopwords:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a3ca93-2608-46b2-a4ee-12edc96742a0",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f147a79-7953-44fa-aff9-388eb2ecb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"Working in difficult weather conditions can be very stressful\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "stemmed_words = []\n",
    "for word in words:\n",
    "  stemmed_words.append(stemmer.stem(word))\n",
    "\n",
    "\" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06bda4b-f8e0-4bc8-8a54-84b79c7d5e4c",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a025e20e-5a74-44a4-bda5-8218b2d3903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = \"Working in difficult weather conditions can be very stressful\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "lemmatized_words = []\n",
    "for word in words:\n",
    "  lemmatized_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "\n",
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74743b0-54a9-41f9-8940-9417a480db7c",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87fffa0-f0e7-4342-b687-89104ab49d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc584ee-c9bc-4d83-b688-d2118ccb812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1afe72d5-86d1-4359-b23a-9417c709a909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'), ('pizza', 'NN'), ('was', 'VBD'), ('overcooked', 'VBN')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"My pizza was overcooked\"\n",
    "\n",
    "tb_text = TextBlob(text)\n",
    "\n",
    "tb_text.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de7f548-700a-4666-ab17-f97c3e2830b0",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "\n",
    "We haven't discussed Neural Networks in this course, but for completeness we need to highlight that Recurent Neural Networks (type of Neural Network) is the state-of-the art approach to solving NLP tasks.\n",
    "\n",
    "A Neural Network, similarly to other classic ML algorithms is a collection of algorithms that aims at uncovering relationships in data. The algorithmic process is inspired by functioning of neuron in human's brain.\n",
    "\n",
    "Recurrent Neural Networks are perfectly suited for:\n",
    "\n",
    "- text classification\n",
    "- text generation\n",
    "\n",
    "RNNs is ideal for solving problems where the sequence is more important than the individual items themselves.\n",
    "\n",
    "## Transformers\n",
    "\n",
    "There are many different Neural Networks that are succesful with solving NLP tasks. One particular neural network - Transformers Neural Network has proven to be especially effective for many NLP tasks.\n",
    "\n",
    "Transformers is a sequence-to-sequence architecture.\n",
    "\n",
    "You can check exemplary Google Colab notebooks and play around yourself: https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/huggingface_pytorch-transformers.ipynb\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eff33e-65ed-4b7c-8c6b-5d8d7224bb2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyladies_venv",
   "language": "python",
   "name": "pyladies_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
